---
title: "Research seminar"
layout: textlay
excerpt: "Artificial Intelligence and Machine Learning Group at  Bundeswehr University Munich."
sitemap: false
permalink: /research-seminar.html
---

## Reseach seminar
Most sessions take place in forms of a reading group or a research presentation by some member of the group or external guest. 

### When and where
Each Wednesday 11:00 - 12:30, hybrid setting <br>
Offline:  Research Institute CODE, Bundeswehr University Munich. <br>
Online: via WebEx. If you are interested, contact <a href="mailto:emmanouil.panagiotou@unibw.de">Emmanouil Panagiotou</a> for the link.


### Who
AIML group's members and everybody who is interested in AI/ML.

### Meetings
<h2>Autumn trimester</h2>
<ul>
<li>13.12.2023<a href="https://www.mi.fu-berlin.de/en/inf/groups/ag-comm/team-members/wunder.html" target="_new">Prof. Gerhard Wunder</a> from FU Berlin gave a talk on "On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box" at the <a href="https://www.unibw.de/code/events/kolloquium" target="_new">CODE colloqium</a> Eirini is co-organizing. We had the opportunity to discuss our research and explore potential collaborations.</li>
<li>22.11.2023<a href="https://imada.sdu.dk/u/zimek/index.html" target="_new">Prof. Arthur Zimek</a> from SDU Denmark gave a talk on "Fairness in Imbalanced Classification: An Adjustment to the k Nearest Neighbor Classifier" at the <a href="https://www.unibw.de/code/events/kolloquium" target="_new">CODE colloqium</a> Eirini is co-organizing. We had the opportunity to discuss our research and explore potential collaborations.</li>
<li>08.11.2023 <b>Vivek</b> will give a summary of his current research on augmenting knowledge.</li>  
<li>31.10.2023 <b>Arjun</b> will give a summary of his current research on the landscape of discrimination in the model parameter space.</li>
  <li>27.09.2023 <b>Valerie Krug</b> from Otto-von-Guericke-University Magdeburg will give a talk on <i>Neuroscience-Inspired Analysis and Visualization of Deep Neural Networks</i>:
  <ul>
    <li>Abstract: Deep Neural Networks (DNNs) are very successful in various fields of application. Their success, however, is mostly achieved by increasing the model complexity in terms of types of architectures or the number of neurons. At the same time, it becomes harder to interpret how DNNs solve their learned task, which can be risky in critical applications. Real brains are even more complex systems which have been studied in neuroscience for decades. This wealth of experience can help to better understand inner workings of DNNs. Therefore, we research how well-established methods from neuroscience can be adapted to analyze and visualize artificial neural networks. In this talk, I present how to analyze DNNs with an Event-Related Potentials-inspired technique and how to visualize their activations as topographic maps.
</li>
    <li>Speaker: Valerie Krug is PhD student on Explainable Artificial Intelligence (XAI). She has a background in natural sciences and holds a Master's degree in Bioinformatics. After her studies, she specialized in Deep Learning during 3 years as a researcher and lecturer at the Machine Learning in Cognitive Science Lab at University of Potsdam. Since 2019 she researches and teaches at the Artificial Intelligence Lab at Otto-von-Guericke-University Magdeburg. Valerie develops techniques to analyze and visualize, how deep neural networks perform their tasks, including novel methods that are inspired by cognitive neuroscience. Her work is driven by the aim to enable non-experts to get insights into inner workings of AI systems. To this end, she is also engaged in educating the public about AI, for instance through exhibits, science quizzes and public talks or discussions.
</li>
  </ul></li>
    <li>13.09.2023 <b>Manolis</b> will give a summary of his current research, which includes two parts:
  <ul>
    <li><i>Learning impartial policies for sequential  counterfactual explanations using Deep Reinforcement Learning</i> (to be presented at the <a href="https://sites.google.com/view/dynxai-ecmlpkdd-2023/home?authuser=0" target="_new">DynXAI Workshop</a>, co-located with ECML PKDD 2023).</li>
    <li><i>Generating synthetic design alternatives in mixed feature spaces - Application on offshore substructure data</i> (related to the <a href="https://www.sfb1463.uni-hannover.de/" target="_new">SFB 1463</a> project)</li>
  </ul>
</li>
</ul>

<h2>Previous sessions</h2>
<ul>
<li>04.07.2023 <a href="https://katesmithmiles.wixsite.com/home" target="_new">Prof. Kate Smiles</a> will give an invited talk on "Stress-testing Algorithms via Instance Space Analysis".</li>
<li>31.05.2023 Invited speaker Tobias will talk about "Compressive Sensing and Geometry of Inverse Problems for Sparse Signals" and Siamak will present his recent accepted paper (EWAF 2023) "Affinity Clustering Framework for Data Debiasing using Pairwise Distribution Discrepancy" </li>
<li>24.05.2023 Manolis will present his work on working with mixed data </li>
<li>17.05.2023 Swati will present her work "Lifelong News Bias Prediction in low-resource multilingual headlines"</li>
<li>10.05.2023 Tai Le Quy will present his new PAKDD 2023 paper <a href = "https://doi.org/10.1007/978-3-031-33374-3_40">"Multi-fair capacitated students-topics grouping problem"</a></li>
<li>29.03.2023 Vivek Kumar will talk about "Knowledge Augmenting Practices for domain adaptation in language models".<br>
<!--<i>Abstract:</i> <small>Using superior algorithms and complex architectures in language models has successfully imparted human-like abilities to machines for specific tasks. But two significant constraints, the available training data size and the understanding of domain-specific context, hamper the pre-trained language models from optimal and reliable performance. A potential solution to tackle these limitations is to equip the language models with domain knowledge. While the commonly adopted techniques use Knowledge Graphs Embeddings (KGEs) to inject domain knowledge, we provide a Knowledge Language Model (K-LM) to use the Resource Description Framework (RDF) triples directly, extracted from world knowledge bases. The proposed model works in conjunction with Generative Pretrained Transformer (GPT-2) and Bidirectional Encoder Representations from Transformers (BERT) and uses a well-defined pipeline to select, categorize, and filter the RDF triples. In addition, we introduce heuristic methods to inject domain-specific knowledge in K-LM, leveraging knowledge graphs (KGs). We tested our approaches on the classification task within the scholarly domain using two KGs, and our results show that our proposed language model has significantly outperformed the baselines and BERT for each KG. Our experimental findings also help us conclude the importance of relevance of KG used over the quantity of injected RDF triples. Also, each of our proposed methods for injecting the RDF triples has increased the overall modelâ€™s accuracy, demonstrating that K-LM is a potential choice for domain adaptation to solve knowledge-driven problems.</small>-->
  </li>
<li>22.03.2023 Paper discussion: Friedrich, F. et al (2023). <a href="https://arxiv.org/pdf/2302.10893.pdf">Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness</a>.</li>
<li>15.03.2023 <a href="https://givasile.github.io/" target="_new">Vasilis Gkolemis</a> will present his paper "<a href="https://givasile.github.io/assets/pdf/gkolemis22_dale.pdf">DALE: Differential Accumulated Local Effects for efficient and accurate global explanations</a>", accepted at ACML22.</li>
<!--<li>08.03.2023  Chethan Krishnamurthy Ramanaik will continue the discussion on the topological structures of the input space in latent representations of autoencoders.</li>-->
<li>01.03.2023 Paper discussion: Hinton, G. (2022). <a href="https://www.cs.toronto.edu/~hinton/FFA13.pdf">The forward-forward algorithm: Some preliminary investigations</a>.</li>
<li>21.02.2023 Chethan Krishnamurthy Ramanaik will present his master thesis "Topological Analysis of Curved Spaces and Hybridization of Autoencoders".</li>
</ul>
